{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1660542b",
   "metadata": {},
   "source": [
    "# EIGO: Evolutionary Image Generation Optimization\n",
    "\n",
    "This notebook runs the EIGO framework on one user-provided prompt to improve image quality and prompt–image alignment without retraining the generator.\n",
    "\n",
    "## What it does\n",
    "\n",
    "* **Generator:** SDXL Turbo (1–4 steps; fast, low cost).\n",
    "* **Search:** you can choose between the recommended **sep-CMA-ES** or **Adam**.\n",
    "* **Fitness:** $(F = a\\times\\hat S_{\\text{aest}} + b\\times\\hat S_{\\text{clip}})$ with $(a,b\\in[0,1])$.\n",
    "* **Output:** best image, scores, and the optimized prompt embeddings.\n",
    "\n",
    "## Typical settings\n",
    "\n",
    "* Steps: 1–4; Guidance scale: 0; Size: $(512\\times512)$.\n",
    "* Weights: $(a,b\\in[0,1])$, where $a+b=1$.\n",
    "* sep-CMA-ES: population $(\\lambda)$, generations $(T)$, step size $(\\sigma)$.\n",
    "* Adam: iterations $(T)$, $(\\alpha)$, $((\\beta_1,\\beta_2))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "selected_prompt = \"an astrounat riding a horse in mars\"\n",
    "\n",
    "optimization_method = \"adam\" #\"cmaes\" for CMA-ES variants, \"adam\" for Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general parameters\n",
    "seed = 42  # Default seed value for reproducibility\n",
    "prompt_per_categorie = 3  # Number of prompts to sample per category from the dataset\n",
    "prompt_sample_seed = 42  # Seed for sampling prompts from the dataset\n",
    "cuda = 0  # GPU device ID to use (e.g., 0 for the first GPU). If no GPU is available, falls back to CPU.\n",
    "predictor = 2  # Predictor type= 0 for SAM, 1 for LAION V1, 2 for LAION V2\n",
    "num_inference_steps = 1  # Number of inference steps for the diffusion model during image generation\n",
    "guidance_scale = 0  # Guidance scale for classifier-free guidance (set to 0 for no guidance)\n",
    "height = 512  # Height of the generated image in pixels\n",
    "width = 512  # Width of the generated image in pixels\n",
    "alpha = 0.7  # Weight for the aesthetic score in the combined fitness function\n",
    "beta = 0.3  # Weight for the CLIP score in the combined fitness function\n",
    "max_aesthetic_score = 10  # Maximum possible aesthetic score (used for normalization)\n",
    "max_clip_score = 0.5  # Maximum possible CLIP score (used for normalization)\n",
    "model_id = \"stabilityai/sdxl-turbo\"  # Model ID for the Stable Diffusion XL pipeline\n",
    "results_folder = \"results\"  # Folder to save results, including\n",
    "\n",
    "# CMA-ES specific parameters (if selected)\n",
    "num_generations = 100  # Number of generations for the CMA-ES optimization process\n",
    "pop_size = 20  # Population size for CMA-ES optimization\n",
    "cmaes_variant = \"sep\"  # Variant of CMA-ES to use: \"cmaes\" (standard), \"sep\" (sep-CMA-ES), or \"vd\" (VD-CMA)\n",
    "sigma = 0.5  # Initial standard deviation for CMA-ES optimization\n",
    "\n",
    "# Adam specific parameters (if selected)\n",
    "num_iterations = 100  # Number of iterations for the Adam optimization process\n",
    "adam_lr = 5e-3  # Learning rate for the Adam optimizer\n",
    "adam_weight_decay = 1e-05  # Weight decay for the Adam optimizer\n",
    "adam_eps = 1e-8  # Epsilon value for numerical stability in the Adam\n",
    "adam_beta1 = 0.85  # Beta1 parameter for the Adam optimizer\n",
    "adam_beta2 = 0.98  # Beta2 parameter for the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup and function definitions\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "import cma\n",
    "from cma.restricted_gaussian_sampler import GaussVDSampler \n",
    "from datasets import load_dataset\n",
    "import clip\n",
    "import argparse\n",
    "\n",
    "if predictor == 0:\n",
    "    predictor_name = 'simulacra'\n",
    "elif predictor == 1:\n",
    "    predictor_name = 'laionv1'\n",
    "elif predictor == 2:\n",
    "    predictor_name = 'laionv2'\n",
    "else:\n",
    "    raise ValueError(\"Invalid predictor option.\")\n",
    "\n",
    "if optimization_method == \"adam\":\n",
    "    method_save_name = \"adam\"\n",
    "elif optimization_method == \"cmaes\":\n",
    "    if cmaes_variant == \"cmaes\":\n",
    "        method_save_name = \"cmaes\"\n",
    "    elif cmaes_variant == \"sep\":\n",
    "        method_save_name = \"sepcmaes\"\n",
    "    elif cmaes_variant == \"vd\":\n",
    "        method_save_name = \"vdcmae\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown CMA-ES variant: {cmaes_variant}\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown optimization method: {optimization_method}\")\n",
    "\n",
    "OUTPUT_FOLDER = f\"{results_folder}/{method_save_name}_clip_{predictor_name}_sdxlturbo_{seed}_a{int(alpha*100)}_b{int(beta*100)}\"\n",
    "\n",
    "# Save the selected prompts and their categories to a text file in the results folder\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Check if a GPU is available and if not, use the CPU\n",
    "device = \"cuda:\" + str(cuda) if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the SDXL pipeline\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "call_with_grad = pipe.__class__.__call__.__wrapped__.__get__(pipe, pipe.__class__)\n",
    "\n",
    "clip_model_name = \"ViT-L/14\"  # CLIP model name\n",
    "clip_model, clip_preprocess = clip.load(clip_model_name, device=device)\n",
    "\n",
    "# Initialize the aesthetic model\n",
    "if predictor == 0:\n",
    "    from aesthetic_evaluation.src import simulacra_rank_image\n",
    "    aesthetic_model = simulacra_rank_image.SimulacraAesthetic(device)\n",
    "    model_name = 'SAM'\n",
    "elif predictor == 1:\n",
    "    from aesthetic_evaluation.src import laion_rank_image\n",
    "    aesthetic_model = laion_rank_image.LAIONAesthetic(device, clip_model=clip_model_name)\n",
    "    model_name = 'LAIONV1'\n",
    "elif predictor == 2:\n",
    "    from aesthetic_evaluation.src import laion_v2_rank_image\n",
    "    aesthetic_model = laion_v2_rank_image.LAIONV2Aesthetic(device, clip_model=clip_model_name)\n",
    "    model_name = 'LAIONV2'\n",
    "else:\n",
    "    raise ValueError(\"Invalid predictor option.\")\n",
    "\n",
    "def generate_image_from_embeddings_cmaes(prompt_embeds, pooled_prompt_embeds, seed):\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "    out = pipe(\n",
    "        prompt_embeds              = prompt_embeds,\n",
    "        pooled_prompt_embeds       = pooled_prompt_embeds,\n",
    "        guidance_scale             = 0.0,\n",
    "        num_inference_steps        = num_inference_steps,\n",
    "        generator                  = generator,\n",
    "        height                     = height,\n",
    "        width                      = width,\n",
    "        output_type               = \"pt\"\n",
    "    )[\"images\"]\n",
    "\n",
    "    image = out.clamp(0, 1).squeeze(0).permute(1, 2, 0)      # HWC\n",
    "    return image.to(device)\n",
    "\n",
    "def generate_image_from_embeddings_adam(text_embeddings, seed):\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "    prompt_embeds = text_embeddings[0]\n",
    "    pooled_prompt_embeds = text_embeddings[1]\n",
    "\n",
    "    out = call_with_grad(\n",
    "        prompt_embeds              = prompt_embeds,\n",
    "        pooled_prompt_embeds       = pooled_prompt_embeds,\n",
    "        guidance_scale             = 0.0,\n",
    "        num_inference_steps        = num_inference_steps,\n",
    "        generator                  = generator,\n",
    "        height                     = height,\n",
    "        width                      = width,\n",
    "        output_type               = \"pt\"\n",
    "    )[\"images\"]\n",
    "\n",
    "    image = out.clamp(0, 1).squeeze(0).permute(1, 2, 0)      # HWC\n",
    "    return image.to(device)\n",
    "\n",
    "def aesthetic_evaluation(image):\n",
    "    # image is a tensor of shape [H, W, C]\n",
    "    # Convert to [N, C, H, W] and ensure it's in float32\n",
    "    image_input = image.permute(2, 0, 1).to(torch.float32)  # [1, C, H, W]\n",
    "\n",
    "    if predictor == 0:\n",
    "        # Simulacra Aesthetic Model\n",
    "        score = aesthetic_model.predict_from_tensor(image_input)\n",
    "    elif predictor == 1 or predictor == 2:\n",
    "        # LAION Aesthetic Predictor V1 and V2\n",
    "        score = aesthetic_model.predict_from_tensor(image_input)\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    return score\n",
    "\n",
    "def evaluate_clip_score_cmaes(image_tensor, prompt):\n",
    "    # Convert the image tensor to a PIL image\n",
    "    image = (image_tensor * 255).clamp(0, 255).byte()\n",
    "    image = Image.fromarray(image.cpu().numpy())\n",
    "\n",
    "    # Preprocess the image\n",
    "    image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    text_input = clip.tokenize([prompt]).to(device)\n",
    "\n",
    "    # Compute the CLIP embeddings\n",
    "    image_features = clip_model.encode_image(image_input)\n",
    "    text_features = clip_model.encode_text(text_input)\n",
    "\n",
    "    # Normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute the cosine similarity (CLIP score)\n",
    "    clip_score = (image_features @ text_features.T)\n",
    "\n",
    "    return clip_score\n",
    "\n",
    "# Differentiable CLIP score evaluation for Adam\n",
    "_CLIP_MEAN = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(1,3,1,1)\n",
    "_CLIP_STD  = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(1,3,1,1)\n",
    "def evaluate_clip_score_adam(image_tensor, text_features):\n",
    "    clip_model.eval()\n",
    "\n",
    "    # --- differentiable preprocess (no PIL, no .byte) ---\n",
    "    img = image_tensor.permute(2,0,1).unsqueeze(0)            # [1,C,H,W]\n",
    "    img = img.to(device=device, dtype=torch.float32)\n",
    "    img = F.interpolate(img, size=(224,224), mode=\"bicubic\", align_corners=False)\n",
    "    mean = _CLIP_MEAN.to(img.device, img.dtype)\n",
    "    std  = _CLIP_STD.to(img.device, img.dtype)\n",
    "    img = (img - mean) / std\n",
    "\n",
    "    # Encode image WITH grad (through CLIP image tower)\n",
    "    image_features = clip_model.encode_image(img).float()\n",
    "    image_features = F.normalize(image_features, dim=-1, eps=1e-6)\n",
    "\n",
    "    sim = (image_features @ text_features.T).squeeze()  # scalar\n",
    "    return sim\n",
    "\n",
    "def format_time(seconds):\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {seconds}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {seconds}s\"\n",
    "    else:\n",
    "        return f\"{seconds}s\"\n",
    "\n",
    "def evaluate(input_embedding, seed, embedding_shape, selected_prompt, save_path=None):\n",
    "    # x is a NumPy array representing the embedding vector\n",
    "    # Convert it to a torch tensor\n",
    "\n",
    "    # Reshape the embedding to the original shape\n",
    "    split = np.prod(embedding_shape[0])\n",
    "    pe  = torch.tensor(input_embedding[:split],  dtype=torch.float32, device=device).view(embedding_shape[0])\n",
    "    ppe = torch.tensor(input_embedding[split:], dtype=torch.float32, device=device).view(embedding_shape[1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image = generate_image_from_embeddings_cmaes(pe, ppe, seed)\n",
    "\n",
    "        aesthetic_score = aesthetic_evaluation(image).item()\n",
    "        clip_score = evaluate_clip_score_cmaes(image, selected_prompt).item()\n",
    "    # CMA-ES minimizes the function, so we need to invert the score if higher is better\n",
    "\n",
    "    fitness_1 = alpha*aesthetic_score/max_aesthetic_score\n",
    "    fitness_2 = beta*clip_score/max_clip_score\n",
    "\n",
    "    fitness = fitness_1 + fitness_2\n",
    "\n",
    "    if save_path is not None:\n",
    "        # Save the generated image\n",
    "        image_np = image.detach().clone().cpu().numpy()\n",
    "        image_np = (image_np * 255).astype(np.uint8)\n",
    "        pil_image = Image.fromarray(image_np)\n",
    "        pil_image.save(save_path)\n",
    "\n",
    "    return -fitness, aesthetic_score, clip_score, fitness_1, fitness_2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf66576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CMA-ES optimization loop definition\n",
    "def main_cmaes(seed, selected_prompt):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    print(f\"Selected prompt: {selected_prompt}\")\n",
    "\n",
    "    results_folder = f\"{OUTPUT_FOLDER}/results_{model_name}_{seed}\"\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        (prompt_embeds,\n",
    "        negative_prompt_embeds,\n",
    "        pooled_prompt_embeds,\n",
    "        negative_pooled_prompt_embeds) = pipe.encode_prompt(\n",
    "                selected_prompt,\n",
    "                negative_prompt=\"\",                 \n",
    "                device=device,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=False\n",
    "        )\n",
    "\n",
    "    # Set CMA-ES options\n",
    "    es_options = {\n",
    "        'seed': seed,\n",
    "        'popsize': pop_size,\n",
    "        'maxiter': num_generations,\n",
    "        'verb_filenameprefix': results_folder + '/outcmaes',  # Save logs\n",
    "        'verb_log': 0,  # Disable log output\n",
    "        'verbose': -9,  # Suppress console output\n",
    "    }\n",
    "\n",
    "    if cmaes_variant == \"cmaes\":\n",
    "        print(\"Using standard CMA-ES\")\n",
    "    elif cmaes_variant == \"sep\":\n",
    "        print(\"Using sep-CMA-ES\")\n",
    "        es_options['CMA_diagonal'] = True\n",
    "    elif cmaes_variant == \"vd\":\n",
    "        print(\"Using VD-CMA-ES\")\n",
    "        es_options = GaussVDSampler.extend_cma_options(es_options)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown CMA-ES variant: {cmaes_variant}\")\n",
    "\n",
    "    trainable_params_init = torch.cat([\n",
    "        prompt_embeds.flatten(),\n",
    "        pooled_prompt_embeds.flatten()\n",
    "        ]).cpu().numpy()\n",
    "\n",
    "    sh_prompt  = prompt_embeds.shape         \n",
    "    sh_pooled  = pooled_prompt_embeds.shape  \n",
    "\n",
    "    text_embeddings_init_shape = [sh_prompt, sh_pooled]\n",
    "\n",
    "    es = cma.CMAEvolutionStrategy(trainable_params_init, sigma, es_options)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        initial_image = generate_image_from_embeddings_cmaes(prompt_embeds.clone(), pooled_prompt_embeds.clone(), seed)\n",
    "        image_np = initial_image.detach().clone().cpu().numpy()\n",
    "        image_np = (image_np * 255).astype(np.uint8)\n",
    "        pil_image = Image.fromarray(image_np)\n",
    "        pil_image.save(f\"{results_folder}/it_0.png\")\n",
    "\n",
    "        initial_fitness, initial_aesthetic_score, initial_clip_score, initial_fitness_1, initial_fitness_2 = evaluate(trainable_params_init, seed, text_embeddings_init_shape, selected_prompt)\n",
    "\n",
    "    time_list = [0]\n",
    "    best_aesthetic_score_overall = initial_aesthetic_score\n",
    "    best_clip_score_overall = initial_clip_score\n",
    "    best_fitness_overall = initial_fitness\n",
    "    best_text_embeddings_overall = trainable_params_init\n",
    "\n",
    "    start_time = time.time()\n",
    "    generation = 0\n",
    "\n",
    "    max_fit_list = [-initial_fitness]\n",
    "    avg_fit_list = [-initial_fitness]\n",
    "    std_fit_list = [0]\n",
    "\n",
    "    max_aesthetic_score_list = [initial_aesthetic_score]\n",
    "    avg_aesthetic_score_list = [initial_aesthetic_score]\n",
    "    std_aesthetic_score_list = [0]\n",
    "\n",
    "    max_clip_score_list = [initial_clip_score]\n",
    "    avg_clip_score_list = [initial_clip_score]\n",
    "    std_clip_score_list = [0]\n",
    "\n",
    "    max_fitness_1_list = [initial_fitness_1]\n",
    "    avg_fitness_1_list = [initial_fitness_1]\n",
    "    std_fitness_1_list = [0]\n",
    "\n",
    "    max_fitness_2_list = [initial_fitness_2]\n",
    "    avg_fitness_2_list = [initial_fitness_2]\n",
    "    std_fitness_2_list = [0]\n",
    "\n",
    "    while not es.stop():\n",
    "        print(f\"Generation {generation+1}/{num_generations}\")\n",
    "\n",
    "        os.makedirs(results_folder+\"/gen_%d\" % (generation+1), exist_ok=True)\n",
    "\n",
    "        # Ask for new candidate solutions\n",
    "        solutions = es.ask()\n",
    "        # Evaluate candidate solutions\n",
    "        tmp_fitnesses = []\n",
    "        tmp_fitness_1 = []\n",
    "        tmp_fitness_2 = []\n",
    "        aesthetic_scores = []\n",
    "        clip_scores = []\n",
    "\n",
    "        ind_id = 1\n",
    "        for x in solutions:\n",
    "            save_path = results_folder + \"/gen_%d/id_%d.png\" % (generation+1, ind_id)\n",
    "            fitness, aesthetic_score, clip_score, fitness_1, fitness_2 = evaluate(x, seed, text_embeddings_init_shape, selected_prompt, save_path)\n",
    "            tmp_fitnesses.append(fitness)\n",
    "            tmp_fitness_1.append(fitness_1)\n",
    "            tmp_fitness_2.append(fitness_2)\n",
    "            aesthetic_scores.append(aesthetic_score)\n",
    "            clip_scores.append(clip_score)\n",
    "            ind_id += 1\n",
    "        # Tell CMA-ES the fitnesses\n",
    "        es.tell(solutions, tmp_fitnesses)\n",
    "\n",
    "        # Record statistics\n",
    "        fitnesses = [-f for f in tmp_fitnesses]  # Convert back to positive scores\n",
    "\n",
    "        max_fit = max(fitnesses)\n",
    "        avg_fit = np.mean(fitnesses)\n",
    "        std_fit = np.std(fitnesses)\n",
    "\n",
    "        max_fit_list.append(max_fit)\n",
    "        avg_fit_list.append(avg_fit)\n",
    "        std_fit_list.append(std_fit)\n",
    "\n",
    "        max_aesthetic_score = max(aesthetic_scores)\n",
    "        avg_aesthetic_score = np.mean(aesthetic_scores)\n",
    "        std_aesthetic_score = np.std(aesthetic_scores)\n",
    "\n",
    "        max_aesthetic_score_list.append(max_aesthetic_score)\n",
    "        avg_aesthetic_score_list.append(avg_aesthetic_score)\n",
    "        std_aesthetic_score_list.append(std_aesthetic_score)\n",
    "\n",
    "        max_clip_score = max(clip_scores)\n",
    "        avg_clip_score = np.mean(clip_scores)\n",
    "        std_clip_score = np.std(clip_scores)\n",
    "\n",
    "        max_clip_score_list.append(max_clip_score)\n",
    "        avg_clip_score_list.append(avg_clip_score)\n",
    "        std_clip_score_list.append(std_clip_score)\n",
    "\n",
    "        max_fitness_1 = max(tmp_fitness_1)\n",
    "        avg_fitness_1 = np.mean(tmp_fitness_1)\n",
    "        std_fitness_1 = np.std(tmp_fitness_1)\n",
    "\n",
    "        max_fitness_1_list.append(max_fitness_1)\n",
    "        avg_fitness_1_list.append(avg_fitness_1)\n",
    "        std_fitness_1_list.append(std_fitness_1)\n",
    "\n",
    "        max_fitness_2 = max(tmp_fitness_2)\n",
    "        avg_fitness_2 = np.mean(tmp_fitness_2)\n",
    "        std_fitness_2 = np.std(tmp_fitness_2)\n",
    "\n",
    "        max_fitness_2_list.append(max_fitness_2)\n",
    "        avg_fitness_2_list.append(avg_fitness_2)\n",
    "        std_fitness_2_list.append(std_fitness_2)\n",
    "\n",
    "        # Get best solution so far\n",
    "        best_x = es.result.xbest\n",
    "        best_fitness = -es.result.fbest  # Convert back to positive score\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate and save the best image\n",
    "            split = np.prod(text_embeddings_init_shape[0])\n",
    "            best_pe  = torch.tensor(best_x[:split],  dtype=torch.float32, device=device).view(text_embeddings_init_shape[0])\n",
    "            best_ppe = torch.tensor(best_x[split:], dtype=torch.float32, device=device).view(text_embeddings_init_shape[1])\n",
    "            best_image = generate_image_from_embeddings_cmaes(best_pe, best_ppe, seed)\n",
    "            image_np = best_image.detach().clone().cpu().numpy()\n",
    "            image_np = (image_np * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(image_np)\n",
    "            pil_image.save(results_folder + \"/best_%d.png\" % (generation+1))\n",
    "\n",
    "        if best_fitness > best_fitness_overall:\n",
    "            best_fitness_overall = best_fitness\n",
    "            best_text_embeddings_overall = best_x\n",
    "\n",
    "        generation += 1\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        generations_done = generation\n",
    "        generations_left = num_generations - generations_done\n",
    "        average_time_per_generation = elapsed_time / generations_done\n",
    "        estimated_time_remaining = average_time_per_generation * generations_left\n",
    "\n",
    "        formatted_time_remaining = format_time(estimated_time_remaining)\n",
    "\n",
    "        time_list.append(elapsed_time)\n",
    "\n",
    "        # Save the metrics\n",
    "        results = pd.DataFrame({\n",
    "            \"generation\": list(range(0, generation + 1)),\n",
    "            \"prompt\": [selected_prompt] + [''] * generation,\n",
    "            \"avg_fitness\": avg_fit_list,\n",
    "            \"std_fitness\": std_fit_list,\n",
    "            \"max_fitness\": max_fit_list,\n",
    "            \"avg_aesthetic_score\": avg_aesthetic_score_list,\n",
    "            \"std_aesthetic_score\": std_aesthetic_score_list,\n",
    "            \"max_aesthetic_score\": max_aesthetic_score_list,\n",
    "            \"avg_clip_score\": avg_clip_score_list,\n",
    "            \"std_clip_score\": std_clip_score_list,\n",
    "            \"max_clip_score\": max_clip_score_list,\n",
    "            \"elapsed_time\": time_list\n",
    "        })\n",
    "\n",
    "        results.to_csv(f\"{results_folder}/fitness_results.csv\", index=False, na_rep='nan')\n",
    "\n",
    "        save_plot_results(results, results_folder)\n",
    "\n",
    "        # Print stats\n",
    "        print(f\"Generation {generation}/{num_generations}: Max fitness: {max_fit}, Avg fitness: {avg_fit}, Max aesthetic score: {max_aesthetic_score}, Avg aesthetic score: {avg_aesthetic_score}, Max clip score: {max_clip_score}, Avg clip score: {avg_clip_score}, Estimated time remaining: {formatted_time_remaining}\")\n",
    "\n",
    "    # Save the overall best image\n",
    "    with torch.no_grad():\n",
    "        split = np.prod(text_embeddings_init_shape[0])\n",
    "        best_overall_pe  = torch.tensor(best_text_embeddings_overall[:split],  dtype=torch.float32, device=device).view(text_embeddings_init_shape[0])\n",
    "        best_overall_ppe = torch.tensor(best_text_embeddings_overall[split:], dtype=torch.float32, device=device).view(text_embeddings_init_shape[1])\n",
    "        best_image = generate_image_from_embeddings_cmaes(best_overall_pe, best_overall_ppe, seed)\n",
    "    best_image_np = best_image.detach().cpu().numpy()\n",
    "    best_image_np = (best_image_np * 255).astype(np.uint8)\n",
    "    pil_image = Image.fromarray(best_image_np)\n",
    "    pil_image.save(f\"{results_folder}/best_all.png\")\n",
    "\n",
    "def plot_mean_std(x_axis, m_vec, std_vec, description, title=None, y_label=None, x_label=None):\n",
    "    lower_bound = [M_new - Sigma for M_new, Sigma in zip(m_vec, std_vec)]\n",
    "    upper_bound = [M_new + Sigma for M_new, Sigma in zip(m_vec, std_vec)]\n",
    "\n",
    "    plt.plot(x_axis, m_vec, '--', label=description + \" Avg.\")\n",
    "    plt.fill_between(x_axis, lower_bound, upper_bound, alpha=.3, label=description + \" Avg. ± SD\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    if y_label is not None:\n",
    "        plt.ylabel(y_label)\n",
    "    if x_label is not None:\n",
    "        plt.xlabel(x_label)\n",
    "\n",
    "def save_plot_results(results, results_folder):\n",
    "    # Plot main fitness evolution\n",
    "    plt.figure(figsize=(10, 6))  # Increase figure size\n",
    "    plot_mean_std(results['generation'], results['avg_fitness'], results['std_fitness'], \"Fitness\")\n",
    "    plt.plot(results['generation'], results['max_fitness'], 'r-', label=\"Best Fitness\")\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Fitness')\n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Move legend outside the plot\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.savefig(results_folder + \"/fitness_evolution.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot aesthetic score evolution\n",
    "    plt.figure(figsize=(10, 6))  # Increase figure size\n",
    "    plot_mean_std(results['generation'], results['avg_aesthetic_score'], results['std_aesthetic_score'], \"Population\")\n",
    "    plt.plot(results['generation'], results['max_aesthetic_score'], 'r-', label=\"Best\")\n",
    "    plt.ylim(0, 10)\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Aesthetic Score')\n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Move legend outside the plot\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.savefig(results_folder + \"/aesthetic_score_evolution.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot clip score evolution\n",
    "    plt.figure(figsize=(10, 6))  # Increase figure size\n",
    "    plot_mean_std(results['generation'], results['avg_clip_score'], results['std_clip_score'], \"Population\")\n",
    "    plt.plot(results['generation'], results['max_clip_score'], 'r-', label=\"Best\")\n",
    "    plt.ylim(0, 0.6)\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('CLIP Score')\n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Move legend outside the plot\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.savefig(results_folder + \"/clip_score_evolution.png\") \n",
    "    plt.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c3a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam optimization loop definition\n",
    "\n",
    "def main_adam(seed, selected_prompt):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    print(f\"Selected prompt: {selected_prompt}\")\n",
    "\n",
    "    results_folder = f\"{OUTPUT_FOLDER}/results_{model_name}_{seed}\"\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "    # Text features don't depend on your params; compute w/o grad\n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize([selected_prompt]).to(device)\n",
    "        text_features = clip_model.encode_text(text_tokens).float()\n",
    "        text_features = F.normalize(text_features, dim=-1, eps=1e-6)\n",
    "\n",
    "    (prompt_embeds,\n",
    "        negative_prompt_embeds,\n",
    "        pooled_prompt_embeds,\n",
    "        negative_pooled_prompt_embeds) = pipe.encode_prompt(\n",
    "                selected_prompt,\n",
    "                negative_prompt=\"\",                 \n",
    "                device=device,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=False\n",
    "    )\n",
    "    text_embeddings_init = [prompt_embeds.detach().clone(), pooled_prompt_embeds.detach().clone()]\n",
    "    text_embeddings = [torch.nn.Parameter(prompt_embeds.clone()), torch.nn.Parameter(pooled_prompt_embeds.clone())]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        initial_image = generate_image_from_embeddings_adam(text_embeddings_init, seed)\n",
    "        image_np = initial_image.detach().clone().cpu().numpy()\n",
    "        image_np = (image_np * 255).astype(np.uint8)\n",
    "        pil_image = Image.fromarray(image_np)\n",
    "        pil_image.save(f\"{results_folder}/it_0.png\")\n",
    "\n",
    "    aesthetic_score = aesthetic_evaluation(initial_image)\n",
    "\n",
    "    clip_score = evaluate_clip_score_adam(initial_image, text_features)\n",
    "\n",
    "    initial_combined_score = alpha * aesthetic_score / max_aesthetic_score + beta * clip_score / max_clip_score\n",
    "    initial_combined_loss = 1 - initial_combined_score\n",
    "\n",
    "    combined_score_list = [initial_combined_score.item()]\n",
    "    combined_loss_list = [initial_combined_loss.item()]\n",
    "    time_list = [0]\n",
    "    best_score = initial_combined_score\n",
    "    best_text_embeddings = text_embeddings_init.copy()\n",
    "\n",
    "    optimizer = torch.optim.Adam(text_embeddings, lr=adam_lr, betas=(adam_beta1,adam_beta2), weight_decay=adam_weight_decay, eps=adam_eps)  \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Add lists to store the metrics\n",
    "    aesthetic_score_list = [aesthetic_score.item()]\n",
    "    clip_score_list = [clip_score.item()]\n",
    "\n",
    "    for iteration in range(1, num_iterations + 1):\n",
    "        print(f\"Iteration {iteration}/{num_iterations}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "        image = generate_image_from_embeddings_adam(text_embeddings, seed)\n",
    "        aesthetic_score = aesthetic_evaluation(image)\n",
    "        clip_score = evaluate_clip_score_adam(image, text_features)\n",
    "        combined_score = alpha * aesthetic_score / max_aesthetic_score + beta * clip_score / max_clip_score\n",
    "        combined_loss = 1 - combined_score\n",
    "\n",
    "        # Calculate gradients\n",
    "        combined_loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append metrics to their respective lists\n",
    "        aesthetic_score_list.append(aesthetic_score.item())\n",
    "        clip_score_list.append(clip_score.item())\n",
    "\n",
    "        if combined_score.item() > best_score:\n",
    "            best_score = combined_score.item()\n",
    "            best_text_embeddings = text_embeddings.copy()\n",
    "\n",
    "        combined_score_list.append(combined_score.item())\n",
    "        combined_loss_list.append(combined_loss.item())\n",
    "\n",
    "        image_np = image.detach().clone().cpu().numpy()\n",
    "        image_np = (image_np * 255).astype(np.uint8)\n",
    "        pil_image = Image.fromarray(image_np)\n",
    "        pil_image.save(f\"{results_folder}/it_{iteration}.png\")\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        iterations_done = iteration\n",
    "        iterations_left = num_iterations - iteration\n",
    "        average_time_per_iteration = elapsed_time / iterations_done\n",
    "        estimated_time_remaining = average_time_per_iteration * iterations_left\n",
    "\n",
    "        formatted_time_remaining = format_time(estimated_time_remaining)\n",
    "\n",
    "        time_list.append(elapsed_time)\n",
    "\n",
    "        # Save metrics to the results DataFrame\n",
    "        results = pd.DataFrame({\n",
    "            \"iteration\": list(range(0, iteration + 1)),\n",
    "            \"prompt\": [selected_prompt] + [''] * iteration,\n",
    "            \"combined_score\": combined_score_list,\n",
    "            \"combined_loss\": combined_loss_list,\n",
    "            \"aesthetic_score\": aesthetic_score_list,\n",
    "            \"clip_score\": clip_score_list,\n",
    "            \"elapsed_time\": time_list\n",
    "        })\n",
    "\n",
    "        results.to_csv(f\"{results_folder}/score_results.csv\", index=False, na_rep='nan')\n",
    "\n",
    "        # Plot and save the fitness evolution\n",
    "        plot_results(results, results_folder)\n",
    "\n",
    "        # Print stats\n",
    "        print(f\"Iteration {iteration}/{num_iterations}: Combined Score: {combined_score.item()}, Aesthetic Score: {aesthetic_score.item()}, CLIP Score: {clip_score.item()}, Estimated time remaining: {formatted_time_remaining}\")\n",
    "\n",
    "    # Save the overall best image\n",
    "    with torch.no_grad():\n",
    "        best_image = generate_image_from_embeddings_adam(best_text_embeddings, seed)\n",
    "    best_image_np = best_image.detach().cpu().numpy()\n",
    "    best_image_np = (best_image_np * 255).astype(np.uint8)\n",
    "    pil_image = Image.fromarray(best_image_np)\n",
    "    pil_image.save(f\"{results_folder}/best_all.png\")\n",
    "\n",
    "def plot_results(results, results_folder):\n",
    "    plt.figure(figsize=(10, 6))  # Increase figure size\n",
    "    plt.plot(results['iteration'], results['aesthetic_score'], label=\"Aesthetic Score\")\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Aesthetic Score')\n",
    "    plt.title('Aesthetic Score Evolution')\n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Move legend outside the plot\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.savefig(results_folder + \"/aesthetic_evolution.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))  # Increase figure size\n",
    "    plt.plot(results['iteration'], results['clip_score'], label=\"CLIP Score\")\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('CLIP Score')\n",
    "    plt.title('CLIP Score Evolution')\n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Move legend outside the plot\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.savefig(results_folder + \"/clip_evolution.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot all losses in one plot\n",
    "    plt.figure(figsize=(10, 6))  # Increase figure size\n",
    "    plt.plot(results['iteration'], results['combined_loss'], label=\"Combined Loss\")\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Evolution')\n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Move legend outside the plot\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.savefig(results_folder + \"/loss_evolution.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_mean_std(x_axis, m_vec, std_vec, description, title=None, y_label=None, x_label=None):\n",
    "    lower_bound = [M_new - Sigma for M_new, Sigma in zip(m_vec, std_vec)]\n",
    "    upper_bound = [M_new + Sigma for M_new, Sigma in zip(m_vec, std_vec)]\n",
    "\n",
    "    plt.plot(x_axis, m_vec, '--', label=description + \" Avg.\")\n",
    "    plt.fill_between(x_axis, lower_bound, upper_bound, alpha=.3, label=description + \" Avg. ± SD\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    if y_label is not None:\n",
    "        plt.ylabel(y_label)\n",
    "    if x_label is not None:\n",
    "        plt.xlabel(x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimization execution\n",
    "\n",
    "if optimization_method == \"cmaes\":\n",
    "    main_cmaes(seed, selected_prompt)\n",
    "elif optimization_method == \"adam\":\n",
    "    main_adam(seed, selected_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b5740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eigo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
